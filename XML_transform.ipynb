{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict, json\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "data_path = '/Users/jingweizhang/Documents/Projects/BoE_news_data/'\n",
    "data_hist = 'text_en_BoE_199801_201710'\n",
    "data_recent = 'text_en_BoE_201711_201908'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files_recent = [i for i in os.listdir(data_path + data_recent) if i.endswith('XML')]\n",
    "list_of_files_hist = [i for i in os.listdir(data_path + data_hist) if i.endswith('XML')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_altId(altid_item):\n",
    "    for value in altid_item:\n",
    "        if value['@type'] == \"idType:USN\":\n",
    "            return value['#text']\n",
    "        \n",
    "def xml_to_json(file_name, data_dir, save_output = False):\n",
    "    xml_file = open(data_path + data_dir + '/' + file_name).read()\n",
    "    obj = xmltodict.parse(xml_file)\n",
    "    dict_file = json.loads(json.dumps(obj))\n",
    "    if save_output == True:\n",
    "        with open(data_path + 'json_data/'+ file_name[:-3] + '.json', 'w') as outfile:\n",
    "             json.dump(dict_file, outfile)\n",
    "    return dict_file\n",
    "\n",
    "def json_to_dict_2017(file_name,data_dir):\n",
    "    dict_file = xml_to_json(file_name,data_dir)\n",
    "    news_item = dict_file['newsMessage']['itemSet']['newsItem']         \n",
    "    news_dict = dict()\n",
    "    \n",
    "    news_dict = {'Id': file_name.split('_')[1][:-4],\n",
    "                 'time': dict_file['newsMessage']['header']['sent'],\n",
    "                 'language':news_item['contentMeta']['language']['@tag'],\n",
    "                 'headline':news_item['contentMeta']['headline'],\n",
    "                 'wordcount': int(news_item['contentSet']['inlineXML']['@wordcount']),\n",
    "                 'genre': None,\n",
    "                 'slugline': news_item['contentMeta']['slugline']['#text'],\n",
    "                 'subject':','.join([value['@qcode'] for value in news_item['contentMeta']['subject'] if '@qcode' in value.keys()]),\n",
    "                 'urgency': news_item['contentMeta']['urgency'],\n",
    "                 'altId': get_altId(news_item['contentMeta']['altId'])\n",
    "                 }\n",
    "    if 'genre' in news_item['contentMeta'].keys():\n",
    "        genres = news_item['contentMeta']['genre']\n",
    "        if type(genres) == list:\n",
    "            if \n",
    "            news_dict['genre'] = ','.join([value['name'] for value in genres])\n",
    "        else:\n",
    "            news_dict['genre'] = genres['name'] \n",
    "    \n",
    "    return news_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dict_1998(file_name,data_dir):\n",
    "    dict_file = xml_to_json(file_name,data_dir)\n",
    "    news_item = dict_file['n-docbody']['document']\n",
    "    news_dict = dict()\n",
    "    \n",
    "    ner_item = news_item['indexing']\n",
    "    \n",
    "    if type(ner_item) == list:\n",
    "        ner_item = ner_item[1]\n",
    "        \n",
    "    try:\n",
    "        ric_codes_list = news_item['indexing'][0]['business-terms']['ric-code-wrap']['ric-code']\n",
    "        ric_codes = [value['#text'] for value in ric_codes_list]\n",
    "    except:\n",
    "        ric_codes = None\n",
    "        \n",
    "        \n",
    "    def get_feature_list(classification_feature):\n",
    "        '''\n",
    "        feature: location, subject, industry\n",
    "        \n",
    "        '''\n",
    "        if 'classification-terms' in ner_item.keys():\n",
    "            block = 'pres-%s-block'%(classification_feature)\n",
    "            wrap =  'pres-%s-wrap'%(classification_feature)\n",
    "            feature_list = None\n",
    "            if type(ner_item['classification-terms']) == list:\n",
    "                if block in ner_item['classification-terms'][0].keys():\n",
    "                    feature_list = ner_item['classification-terms'][0][block][wrap]\n",
    "                elif block in ner_item['classification-terms'][1].keys():\n",
    "                    feature_list = ner_item['classification-terms'][1][block][wrap]\n",
    "\n",
    "            elif block in ner_item['classification-terms'].keys():\n",
    "                 feature_list = ner_item['classification-terms'][block][wrap]\n",
    "\n",
    "            if feature_list != None:\n",
    "                if type(feature_list) == list:\n",
    "                    feature_items = [value['pres-%s'%(classification_feature)] for value in feature_list]\n",
    "                else:\n",
    "                    feature_items = feature_list['pres-%s'%(classification_feature)]\n",
    "\n",
    "                return ','.join(feature_items)\n",
    "\n",
    "    news_dict = {'Id':file_name.split('.')[0], \n",
    "                'time': news_item['pub-info']['pub-date']['sort-pub-date'],\n",
    "                 'language': news_item['@load-language'],\n",
    "                 'headline': news_item['title-info']['title']['sort-title'],\n",
    "                 'wordcount': int(news_item['content']['derived-word-count']['#text'].split()[-1]),\n",
    "                 'location':get_feature_list('location'),\n",
    "                 'subject':get_feature_list('subject'),\n",
    "                 'industry':get_feature_list('industry'),\n",
    "                 'ric_codes': ric_codes\n",
    "                 }\n",
    "    \n",
    "    if type(news_dict['headline']) == dict:\n",
    "        news_dict['headline'] = news_dict['headline']['#text']\n",
    "    \n",
    "    return news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_meta_recent = [json_to_dict_2017(file, data_recent) for file in list_of_files_recent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_meta_hist = [json_to_dict_1998(file, data_hist) for file in list_of_files_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1998 = pd.DataFrame(list_of_meta_hist).sort_values(by = ['time'])\n",
    "df_2017 = pd.DataFrame(list_of_meta_recent).sort_values(by = ['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1998.to_csv('199801_201710.csv', index = False)\n",
    "df_2017.to_csv('201711_201908.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['19980107', '19980108', '19980109', '19980113', '19980114', '19980115',\n",
       "       '19980116', '19980211', '19980212', '19980213',\n",
       "       ...\n",
       "       '20170617', '20170802', '20170803', '20170804', '20170805', '20170913',\n",
       "       '20170914', '20170915', '20170916', '20171101'],\n",
       "      dtype='object', name='time', length=1884)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1998.groupby('time').Id.count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
